{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DEEP FOOL ATTACK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import logging\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, Conv2D, MaxPooling2D, Activation, Dropout\n",
    "import numpy as np\n",
    "\n",
    "from art.attacks.evasion import DeepFool\n",
    "from art.estimators.classification import KerasClassifier\n",
    "from art.utils import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, Conv2D, BatchNormalization, MaxPooling2D, Flatten, Dense\n",
    "from tensorflow.keras.losses import sparse_categorical_crossentropy\n",
    "from tensorflow.keras.metrics import sparse_categorical_accuracy\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorpack.dataflow import dataset, BatchData\n",
    "from tensorpack.train import TrainConfig\n",
    "from tensorpack.tfutils.varreplace import remap_variables\n",
    "from tensorpack.tfutils.scope_utils import auto_reuse_variable_scope\n",
    "from tensorpack.callbacks import ModelSaver, MaxSaver, InferenceRunner\n",
    "from tensorflow.keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure a logger to capture ART outputs; these are printed in console and the level of detail is set to INFO\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "handler = logging.StreamHandler()\n",
    "formatter = logging.Formatter(\"[%(levelname)s] %(message)s\")\n",
    "handler.setFormatter(formatter)\n",
    "logger.addHandler(handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Read CIFAR-10 dataset\n",
    "(x_train, y_train), (x_test, y_test), min_, max_ = load_dataset(\"mnist\")\n",
    "\n",
    "# Shuffle and split the dataset\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    x_train, y_train, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "im_shape = x_train[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv1 (Conv2D)              (None, 26, 26, 32)        288       \n",
      "                                                                 \n",
      " bn1 (BatchNormalization)    (None, 26, 26, 32)        128       \n",
      "                                                                 \n",
      " maxpool1 (MaxPooling2D)     (None, 13, 13, 32)        0         \n",
      "                                                                 \n",
      " conv2 (Conv2D)              (None, 11, 11, 64)        18432     \n",
      "                                                                 \n",
      " bn2 (BatchNormalization)    (None, 11, 11, 64)        256       \n",
      "                                                                 \n",
      " maxpool2 (MaxPooling2D)     (None, 5, 5, 64)          0         \n",
      "                                                                 \n",
      " conv3 (Conv2D)              (None, 3, 3, 128)         73728     \n",
      "                                                                 \n",
      " bn3 (BatchNormalization)    (None, 3, 3, 128)         512       \n",
      "                                                                 \n",
      " conv4 (Conv2D)              (None, 1, 1, 256)         294912    \n",
      "                                                                 \n",
      " bn4 (BatchNormalization)    (None, 1, 1, 256)         1024      \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 256)               0         \n",
      "                                                                 \n",
      " fc1 (Dense)                 (None, 20)                5120      \n",
      "                                                                 \n",
      " output (Dense)              (None, 10)                200       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 394,600\n",
      "Trainable params: 393,640\n",
      "Non-trainable params: 960\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Preprocess the data\n",
    "x_train = x_train / 255.0\n",
    "x_test = x_test / 255.0\n",
    "# Define constants\n",
    "IMAGE_SIZE = 28 # Example value, replace with your actual image size\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Input(shape=(IMAGE_SIZE, IMAGE_SIZE, 1), name='input'))  # Input layer\n",
    "\n",
    "model.add(Conv2D(32, (3, 3), use_bias=False, activation='relu', kernel_regularizer=l2(1e-5), name='conv1'))\n",
    "model.add(BatchNormalization(name='bn1'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), name='maxpool1'))  # Reduced max-pooling\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), use_bias=False, activation='relu', kernel_regularizer=l2(1e-5), name='conv2'))\n",
    "model.add(BatchNormalization(name='bn2'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), name='maxpool2'))  # Reduced max-pooling\n",
    "\n",
    "model.add(Conv2D(128, (3, 3), use_bias=False, activation='relu', kernel_regularizer=l2(1e-5), name='conv3'))\n",
    "model.add(BatchNormalization(name='bn3'))\n",
    "\n",
    "model.add(Conv2D(256, (3, 3), use_bias=False, activation='relu', kernel_regularizer=l2(1e-5), name='conv4'))\n",
    "model.add(BatchNormalization(name='bn4'))\n",
    "\n",
    "model.add(Flatten(name='flatten'))\n",
    "model.add(Dense(20, use_bias=False, activation='relu', kernel_regularizer=l2(1e-5), name='fc1'))\n",
    "model.add(Dense(10, use_bias=False, activation='softmax', name='output'))\n",
    "\n",
    "# Print the model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] Inferred 12 hidden layers on Keras classifier.\n",
      "[INFO] Inferred 12 hidden layers on Keras classifier.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 48000 samples\n",
      "Epoch 1/10\n",
      "48000/48000 [==============================] - 5s 105us/sample - loss: 0.1470 - sparse_categorical_accuracy: 0.9572\n",
      "Epoch 2/10\n",
      "48000/48000 [==============================] - 5s 101us/sample - loss: 0.0464 - sparse_categorical_accuracy: 0.9877\n",
      "Epoch 3/10\n",
      "48000/48000 [==============================] - 5s 99us/sample - loss: 0.0314 - sparse_categorical_accuracy: 0.9921\n",
      "Epoch 4/10\n",
      "48000/48000 [==============================] - 5s 99us/sample - loss: 0.0215 - sparse_categorical_accuracy: 0.9945\n",
      "Epoch 5/10\n",
      "48000/48000 [==============================] - 5s 99us/sample - loss: 0.0168 - sparse_categorical_accuracy: 0.9960\n",
      "Epoch 6/10\n",
      "48000/48000 [==============================] - 5s 99us/sample - loss: 0.0129 - sparse_categorical_accuracy: 0.9976\n",
      "Epoch 7/10\n",
      "48000/48000 [==============================] - 5s 99us/sample - loss: 0.0096 - sparse_categorical_accuracy: 0.9987\n",
      "Epoch 8/10\n",
      "48000/48000 [==============================] - 5s 101us/sample - loss: 0.0077 - sparse_categorical_accuracy: 0.9991\n",
      "Epoch 9/10\n",
      "48000/48000 [==============================] - 5s 100us/sample - loss: 0.0065 - sparse_categorical_accuracy: 0.9995\n",
      "Epoch 10/10\n",
      "48000/48000 [==============================] - 5s 99us/sample - loss: 0.0062 - sparse_categorical_accuracy: 0.9994\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer=SGD(learning_rate=0.01, momentum=0.9),\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['sparse_categorical_accuracy']\n",
    ")\n",
    "\n",
    "# Create classifier wrapper\n",
    "classifier = KerasClassifier(model=model, clip_values=(min_, max_))\n",
    "classifier.fit(x_train, y_train, nb_epochs=10, batch_size=128)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] Classifier test accuracy\n",
      "[INFO] Classifier test accuracy\n",
      "[INFO] Accuracy on test samples: 99.01%\n",
      "[INFO] Accuracy on test samples: 99.01%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the classifier on the adversarial samples\n",
    "preds = np.argmax(classifier.predict(x_test), axis=1)\n",
    "acc = np.sum(preds == np.argmax(y_test, axis=1)) / y_test.shape[0]\n",
    "logger.info(\"Classifier test accuracy\")\n",
    "logger.info(\"Accuracy on test samples: %.2f%%\", (acc * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] Create DeepFool attack\n",
      "[INFO] Craft attack on training examples\n",
      "/home/user/anaconda3/envs/attack/lib/python3.10/site-packages/keras/engine/training_v1.py:2079: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n",
      "[WARNING] It seems that the attacked model is predicting probabilities. DeepFool expects logits as model output to achieve its full attack strength.\n",
      "DeepFool: 100%|██████████| 48000/48000 [4:09:26<00:00,  3.21it/s]   \n"
     ]
    }
   ],
   "source": [
    "# Craft adversarial samples with DeepFool\n",
    "logger.info(\"Create DeepFool attack\")\n",
    "adv_crafter = DeepFool(classifier)\n",
    "logger.info(\"Craft attack on training examples\")\n",
    "x_train_adv = adv_crafter.generate(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] Craft attack test examples\n",
      "[WARNING] It seems that the attacked model is predicting probabilities. DeepFool expects logits as model output to achieve its full attack strength.\n",
      "DeepFool: 100%|██████████| 12000/12000 [1:01:28<00:00,  3.25it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "logger.info(\"Craft attack test examples\")\n",
    "x_test_adv = adv_crafter.generate(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] Classifier before adversarial training\n",
      "[INFO] Accuracy on adversarial samples: 10.61%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Evaluate the classifier on the adversarial samples\n",
    "preds = np.argmax(classifier.predict(x_test_adv), axis=1)\n",
    "acc = np.sum(preds == np.argmax(y_test, axis=1)) / y_test.shape[0]\n",
    "logger.info(\"Classifier before adversarial training\")\n",
    "logger.info(\"Accuracy on adversarial samples: %.2f%%\", (acc * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DEFENSE USING ADVERSIAL TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data augmentation: expand the training set with the adversarial samples\n",
    "x_train = np.append(x_train, x_train_adv, axis=0)\n",
    "y_train = np.append(y_train, y_train, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 96000 samples\n",
      "Epoch 1/10\n",
      "96000/96000 [==============================] - 9s 97us/sample - loss: 0.8014 - accuracy: 0.7527\n",
      "Epoch 2/10\n",
      "96000/96000 [==============================] - 9s 95us/sample - loss: 0.3807 - accuracy: 0.8853\n",
      "Epoch 3/10\n",
      "96000/96000 [==============================] - 9s 96us/sample - loss: 0.2872 - accuracy: 0.9131\n",
      "Epoch 4/10\n",
      "96000/96000 [==============================] - 9s 97us/sample - loss: 0.2448 - accuracy: 0.9267\n",
      "Epoch 5/10\n",
      "96000/96000 [==============================] - 9s 97us/sample - loss: 0.2037 - accuracy: 0.9374\n",
      "Epoch 6/10\n",
      "96000/96000 [==============================] - 9s 97us/sample - loss: 0.2021 - accuracy: 0.9394\n",
      "Epoch 7/10\n",
      "96000/96000 [==============================] - 9s 97us/sample - loss: 0.1611 - accuracy: 0.9513\n",
      "Epoch 8/10\n",
      "96000/96000 [==============================] - 9s 96us/sample - loss: 0.1653 - accuracy: 0.9508\n",
      "Epoch 9/10\n",
      "96000/96000 [==============================] - 9s 97us/sample - loss: 0.1540 - accuracy: 0.9556\n",
      "Epoch 10/10\n",
      "96000/96000 [==============================] - 9s 98us/sample - loss: 0.1469 - accuracy: 0.9574\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] Model saved in path: /home/user/.art/data/Adversial_train_model.h5.\n"
     ]
    }
   ],
   "source": [
    "# Retrain the CNN on the extended dataset\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "classifier.fit(x_train, y_train, nb_epochs=10, batch_size=128)\n",
    "classifier.save(\"Adversial_train_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] Model saved in path: /home/user/Desktop/vamsi/White_box_Attack/MNIST_FINAL/Adversial_train_model.h5.\n"
     ]
    }
   ],
   "source": [
    "classifier.save(\"/home/user/Desktop/vamsi/White_box_Attack/MNIST_FINAL/Adversial_train_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/anaconda3/envs/attack/lib/python3.10/site-packages/keras/engine/training_v1.py:2079: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n",
      "[INFO] Classifier with adversarial training\n",
      "[INFO] Accuracy on adversarial samples: 89.33%\n"
     ]
    }
   ],
   "source": [
    "#Evaluate the adversarially trained classifier on the test set\n",
    "preds = np.argmax(classifier.predict(x_test_adv), axis=1)\n",
    "acc = np.sum(preds == np.argmax(y_test, axis=1)) / y_test.shape[0]\n",
    "logger.info(\"Classifier with adversarial training\")\n",
    "logger.info(\"Accuracy on adversarial samples: %.2f%%\", (acc * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DEFENSIVE DISTILLATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-19 16:24:38.293844: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13703 MB memory:  -> device: 0, name: NVIDIA RTX A4000, pci bus id: 0000:19:00.0, compute capability: 8.6\n",
      "/home/user/anaconda3/envs/attack/lib/python3.10/site-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 48000 samples\n",
      "Epoch 1/10\n",
      "48000/48000 [==============================] - 6s 120us/sample - loss: 2.2862 - accuracy: 0.5562\n",
      "Epoch 2/10\n",
      "48000/48000 [==============================] - 5s 109us/sample - loss: 2.2733 - accuracy: 0.8172\n",
      "Epoch 3/10\n",
      "48000/48000 [==============================] - 5s 110us/sample - loss: 2.2682 - accuracy: 0.9107\n",
      "Epoch 4/10\n",
      "48000/48000 [==============================] - 5s 110us/sample - loss: 2.2652 - accuracy: 0.9541\n",
      "Epoch 5/10\n",
      "48000/48000 [==============================] - 5s 111us/sample - loss: 2.2643 - accuracy: 0.9629\n",
      "Epoch 6/10\n",
      "48000/48000 [==============================] - 5s 109us/sample - loss: 2.2637 - accuracy: 0.9679\n",
      "Epoch 7/10\n",
      "48000/48000 [==============================] - 5s 109us/sample - loss: 2.2633 - accuracy: 0.9729\n",
      "Epoch 8/10\n",
      "48000/48000 [==============================] - 5s 109us/sample - loss: 2.2631 - accuracy: 0.9761\n",
      "Epoch 9/10\n",
      "48000/48000 [==============================] - 5s 108us/sample - loss: 2.2629 - accuracy: 0.9779\n",
      "Epoch 10/10\n",
      "48000/48000 [==============================] - 5s 110us/sample - loss: 2.2627 - accuracy: 0.9796\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/anaconda3/envs/attack/lib/python3.10/site-packages/keras/engine/training_v1.py:2057: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates = self.state_updates\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Accuracy:\t97.75000214576721\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/anaconda3/envs/attack/lib/python3.10/site-packages/keras/engine/training_v1.py:2079: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 48000 samples\n",
      "Epoch 1/10\n",
      "48000/48000 [==============================] - 6s 131us/sample - loss: 2.3060 - accuracy: 0.1245\n",
      "Epoch 2/10\n",
      "48000/48000 [==============================] - 6s 119us/sample - loss: 2.3060 - accuracy: 0.1328\n",
      "Epoch 3/10\n",
      "48000/48000 [==============================] - 6s 118us/sample - loss: 2.3060 - accuracy: 0.1396\n",
      "Epoch 4/10\n",
      "48000/48000 [==============================] - 6s 117us/sample - loss: 2.3060 - accuracy: 0.1486\n",
      "Epoch 5/10\n",
      "48000/48000 [==============================] - 6s 118us/sample - loss: 2.3060 - accuracy: 0.1563\n",
      "Epoch 6/10\n",
      "48000/48000 [==============================] - 6s 117us/sample - loss: 2.3059 - accuracy: 0.1630\n",
      "Epoch 7/10\n",
      "48000/48000 [==============================] - 6s 118us/sample - loss: 2.3059 - accuracy: 0.1720\n",
      "Epoch 8/10\n",
      "48000/48000 [==============================] - 6s 119us/sample - loss: 2.3059 - accuracy: 0.1814\n",
      "Epoch 9/10\n",
      "48000/48000 [==============================] - 6s 120us/sample - loss: 2.3059 - accuracy: 0.1901\n",
      "Epoch 10/10\n",
      "48000/48000 [==============================] - 6s 119us/sample - loss: 2.3059 - accuracy: 0.1991\n",
      "Train on 48000 samples\n",
      "Epoch 1/10\n",
      "48000/48000 [==============================] - 6s 128us/sample - loss: 2.3059 - accuracy: 0.2539\n",
      "Epoch 2/10\n",
      "48000/48000 [==============================] - 6s 117us/sample - loss: 2.3058 - accuracy: 0.3863\n",
      "Epoch 3/10\n",
      "48000/48000 [==============================] - 6s 118us/sample - loss: 2.3058 - accuracy: 0.5060\n",
      "Epoch 4/10\n",
      "48000/48000 [==============================] - 6s 117us/sample - loss: 2.3057 - accuracy: 0.5880\n",
      "Epoch 5/10\n",
      "48000/48000 [==============================] - 6s 119us/sample - loss: 2.3057 - accuracy: 0.6460\n",
      "Epoch 6/10\n",
      "48000/48000 [==============================] - 6s 118us/sample - loss: 2.3056 - accuracy: 0.6939\n",
      "Epoch 7/10\n",
      "48000/48000 [==============================] - 6s 118us/sample - loss: 2.3056 - accuracy: 0.7325\n",
      "Epoch 8/10\n",
      "48000/48000 [==============================] - 6s 118us/sample - loss: 2.3055 - accuracy: 0.7712\n",
      "Epoch 9/10\n",
      "48000/48000 [==============================] - 6s 118us/sample - loss: 2.3055 - accuracy: 0.8029\n",
      "Epoch 10/10\n",
      "48000/48000 [==============================] - 6s 118us/sample - loss: 2.3054 - accuracy: 0.8275\n",
      "Distilled Accuracy:\t84.15833115577698\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "import tensorflow.keras.optimizers as optimizers\n",
    "from keras.layers import Dense, Dropout, Flatten, Lambda\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "from scipy.special import softmax\n",
    "import logging\n",
    "tf.get_logger().setLevel(logging.ERROR)\n",
    "\n",
    "config = tf.compat.v1.ConfigProto()\n",
    "\n",
    "config.gpu_options.allow_growth=True\n",
    "sess = tf.compat.v1.Session(config=config)\n",
    "\n",
    "from keras.models import load_model\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.style as style\n",
    "style.use('fivethirtyeight')\n",
    "import pickle as pkl\n",
    "\n",
    "batch_size = 128\n",
    "num_classes = 10\n",
    "epochs = 10\n",
    "img_rows, img_cols = 28, 28\n",
    "# temperature = 20\n",
    "\n",
    "colors = [[0,0,0], [230/255,159/255,0], [86/255,180/255,233/255], [0,158/255,115/255],\n",
    "          [213/255,94/255,0], [0,114/255,178/255]]\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.datasets import mnist\n",
    "\n",
    "# Load the MNIST dataset\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "# Normalize pixel values to be between 0 and 1\n",
    "x_train = x_train / 255.0\n",
    "x_test = x_test / 255.0\n",
    "\n",
    "# Reshape the data to match your model's input shape\n",
    "x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)\n",
    "x_test = x_test.reshape(x_test.shape[0], 28, 28, 1)\n",
    "\n",
    "# Split the dataset into training and testing sets (80% training, 20% testing)\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "y_train = tf.keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
    "    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
    "    input_shape = (1, img_rows, img_cols)\n",
    "else:\n",
    "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
    "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
    "    input_shape = (img_rows, img_cols, 1)\n",
    "\n",
    "#folder = os.path.abspath(os.getcwd())\n",
    "folder= \"/home/user/Desktop/vamsi/White_box_Attack/MNIST_FINAL\"\n",
    "\n",
    "def temperature_cross_entropy(gt, pred):\n",
    "    loss = tf.nn.softmax_cross_entropy_with_logits(labels=gt, logits=pred/20)\n",
    "    return loss\n",
    "\n",
    "# 99.33%\n",
    "def initial_model(train=True, vis=False):\n",
    "    model_name = os.path.join(folder, 'initial_model.h5')\n",
    "    if train:\n",
    "        IMAGE_SIZE = 28 # Example value, replace with your actual image size\n",
    "\n",
    "        model = Sequential()\n",
    "        model.add(Input(shape=(IMAGE_SIZE, IMAGE_SIZE, 1), name='input'))  # Input layer\n",
    "\n",
    "        model.add(Conv2D(32, (3, 3), use_bias=False, activation='relu', kernel_regularizer=l2(1e-5), name='conv1'))\n",
    "        model.add(BatchNormalization(name='bn1'))\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2), name='maxpool1'))  # Reduced max-pooling\n",
    "\n",
    "        model.add(Conv2D(64, (3, 3), use_bias=False, activation='relu', kernel_regularizer=l2(1e-5), name='conv2'))\n",
    "        model.add(BatchNormalization(name='bn2'))\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2), name='maxpool2'))  # Reduced max-pooling\n",
    "\n",
    "        model.add(Conv2D(128, (3, 3), use_bias=False, activation='relu', kernel_regularizer=l2(1e-5), name='conv3'))\n",
    "        model.add(BatchNormalization(name='bn3'))\n",
    "\n",
    "        model.add(Conv2D(256, (3, 3), use_bias=False, activation='relu', kernel_regularizer=l2(1e-5), name='conv4'))\n",
    "        model.add(BatchNormalization(name='bn4'))\n",
    "\n",
    "        model.add(Flatten(name='flatten'))\n",
    "        model.add(Dense(20, use_bias=False, activation='relu', kernel_regularizer=l2(1e-5), name='fc1'))\n",
    "        model.add(Dense(10, use_bias=False, activation='softmax', name='output'))\n",
    "\n",
    "\n",
    "        sgd = optimizers.SGD(lr=0.01, momentum=0.9, nesterov=True)\n",
    "        model.compile(loss=temperature_cross_entropy, optimizer=sgd, metrics=['accuracy'])\n",
    "        history = model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1)\n",
    "\n",
    "        with open(model_name[:-3]+'_history.h5', 'wb') as f:\n",
    "            pkl.dump(history.history, f)\n",
    "        model.save(model_name)\n",
    "\n",
    "    if vis:\n",
    "        with open(model_name[:-3]+'_history.h5', 'rb') as f:\n",
    "            history = pkl.load(f)\n",
    "        plt.plot(history['loss'], color=[0,158/255,115/255])\n",
    "        plt.title('Initial Model Loss')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.tight_layout()\n",
    "        # plt.legend(['train'], loc='upper right')\n",
    "        plt.savefig('initial_model_loss.png')\n",
    "        plt.close()\n",
    "        # plt.show()\n",
    "\n",
    "    model = load_model(model_name, custom_objects={'temperature_cross_entropy': temperature_cross_entropy})\n",
    "    score = model.evaluate(x_test, y_test, verbose=0)\n",
    "    print(f'Original Accuracy:\\t{score[1]*100}')\n",
    "\n",
    "# 99.49%\n",
    "def distilled_model(train=True, vis=False):\n",
    "    model_name = os.path.join(folder, 'distilled_model.h5')\n",
    "    if train:\n",
    "        initial_model = load_model(os.path.join(folder, 'initial_model.h5'),\n",
    "            custom_objects={'temperature_cross_entropy': temperature_cross_entropy})\n",
    "        y_train = initial_model.predict(x_train, batch_size=batch_size)\n",
    "        y_train = softmax(y_train/20, axis=1)\n",
    "\n",
    "        IMAGE_SIZE = 28 # Example value, replace with your actual image size\n",
    "\n",
    "        model = Sequential()\n",
    "        model.add(Input(shape=(IMAGE_SIZE, IMAGE_SIZE, 1), name='input'))  # Input layer\n",
    "\n",
    "        model.add(Conv2D(32, (3, 3), use_bias=False, activation='relu', kernel_regularizer=l2(1e-5), name='conv1'))\n",
    "        model.add(BatchNormalization(name='bn1'))\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2), name='maxpool1'))  # Reduced max-pooling\n",
    "\n",
    "        model.add(Conv2D(64, (3, 3), use_bias=False, activation='relu', kernel_regularizer=l2(1e-5), name='conv2'))\n",
    "        model.add(BatchNormalization(name='bn2'))\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2), name='maxpool2'))  # Reduced max-pooling\n",
    "\n",
    "        model.add(Conv2D(128, (3, 3), use_bias=False, activation='relu', kernel_regularizer=l2(1e-5), name='conv3'))\n",
    "        model.add(BatchNormalization(name='bn3'))\n",
    "\n",
    "        model.add(Conv2D(256, (3, 3), use_bias=False, activation='relu', kernel_regularizer=l2(1e-5), name='conv4'))\n",
    "        model.add(BatchNormalization(name='bn4'))\n",
    "\n",
    "        model.add(Flatten(name='flatten'))\n",
    "        model.add(Dense(20, use_bias=False, activation='relu', kernel_regularizer=l2(1e-5), name='fc1'))\n",
    "        model.add(Dense(10, use_bias=False, activation='softmax', name='output'))\n",
    "\n",
    "\n",
    "        sgd = optimizers.SGD(lr=0.01, momentum=0.9, nesterov=True)\n",
    "        model.compile(loss=temperature_cross_entropy, optimizer=sgd, metrics=['accuracy'])\n",
    "        history = model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1)\n",
    "\n",
    "        sgd = optimizers.SGD(lr=0.1,momentum=0.9, nesterov=True)\n",
    "        model.compile(loss=temperature_cross_entropy, optimizer=sgd, metrics=['accuracy'])\n",
    "        history = model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1)\n",
    "\n",
    "        with open(model_name[:-3]+'_history.h5', 'wb') as f:\n",
    "            pkl.dump(history.history, f)\n",
    "        model.save(model_name)\n",
    "\n",
    "    if vis:\n",
    "        with open(model_name[:-3]+'_history.h5', 'rb') as f:\n",
    "            history = pkl.load(f)\n",
    "        plt.plot(history['loss'], color=[0,158/255,115/255])\n",
    "        plt.title('Distilled Model Loss')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.tight_layout()\n",
    "        # plt.legend(['train'], loc='upper right')\n",
    "        plt.savefig('distilled_model_loss.png')\n",
    "        # plt.show()\n",
    "\n",
    "    model = load_model(model_name, custom_objects={'temperature_cross_entropy': temperature_cross_entropy})\n",
    "    score = model.evaluate(x_test, y_test, verbose=0)\n",
    "    print(f'Distilled Accuracy:\\t{score[1]*100}')\n",
    "\n",
    "\n",
    "def Main():\n",
    "    initial_model(train=True, vis=False)\n",
    "    distilled_model(train=True, vis=False)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    Main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distilled Accuracy:\t84.15833115577698\n"
     ]
    }
   ],
   "source": [
    "model_name = os.path.join(folder, 'distilled_model.h5')\n",
    "model = load_model(model_name, custom_objects={'temperature_cross_entropy': temperature_cross_entropy})\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(f'Distilled Accuracy:\\t{score[1]*100}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distilled Accuracy:\t16.608333587646484\n"
     ]
    }
   ],
   "source": [
    "model_name = os.path.join(folder, 'distilled_model.h5')\n",
    "model = load_model(model_name, custom_objects={'temperature_cross_entropy': temperature_cross_entropy})\n",
    "score = model.evaluate(x_test_adv, y_test, verbose=0)\n",
    "print(f'Distilled Accuracy:\\t{score[1]*100}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "attack",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
